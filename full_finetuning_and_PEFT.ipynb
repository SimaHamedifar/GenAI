{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b6476db",
   "metadata": {},
   "source": [
    "Fine tune FLAN-T5 model for dialogue summerization task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42392986",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
    "import torch\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad32526c",
   "metadata": {},
   "source": [
    "Load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002091e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "dataset = load_dataset(huggingface_dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be12d928",
   "metadata": {},
   "source": [
    "Load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768403c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"google/flan-t5-base\"\n",
    "model_name = \"google/flan-t5-small\"\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a7f00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae9c209",
   "metadata": {},
   "source": [
    "Pull out the model parameters and find out how many of them are trained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5881d05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_params}\\nall model parameters: {all_params}\\nprecentage of trainable model parameters: {(trainable_params/all_params)*100}%\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02729095",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ef8c7c",
   "metadata": {},
   "source": [
    "Test the model with zero-shot inferencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a81cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 200\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "\n",
    "Prompt = f\"\"\"\n",
    "Summarize the following conversation:\n",
    "{dialogue}\n",
    "Summary:\n",
    "\"\"\"\n",
    "inputs = tokenizer(Prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    original_model.generate(inputs[\"input_ids\"],\n",
    "                            max_new_tokens = 200,\n",
    "                            )[0],\n",
    "                            skip_special_tokens=True\n",
    ")\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{Prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY: \\n{summary}')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION-ZERO SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e438ae8",
   "metadata": {},
   "source": [
    "Perfrom full fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547a8884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    start_prompt = 'Summarize the following conversation.\\n\\n'\n",
    "    end_prompt = '\\n\\nSummary: '\n",
    "    prompt = [start_prompt+dialogue+end_prompt for dialogue in example[\"dialogue\"]]\n",
    "    example[\"input_ids\"] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors='pt').input_ids\n",
    "    example[\"labels\"] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors='pt').input_ids\n",
    "    return example\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(['id', 'topic', 'dialogue', 'summary'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07f81dd",
   "metadata": {},
   "source": [
    "A subsample of the tokenized dataset is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf73bcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.filter(lambda example, index: index % 100 == 0, with_indices=True)\n",
    "print(\"Shapes of the datasets:\\n\")\n",
    "print(f\"Train: {tokenized_dataset['train'].shape}\")\n",
    "print(f\"Test: {tokenized_dataset['test'].shape}\")\n",
    "print(f\"Validation: {tokenized_dataset['validation'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0547e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f'./dialogue-summary-training-{str(int(time.time()))}'\n",
    "training_args = TrainingArguments(output_dir=output_dir, \n",
    "                                  learning_rate=1e-5,\n",
    "                                  num_train_epochs=3,\n",
    "                                  weight_decay=0.01,\n",
    "                                  logging_steps=10,\n",
    "                                  max_steps=1)\n",
    "trainer = Trainer(model=original_model,\n",
    "                  args=training_args,\n",
    "                  train_dataset=tokenized_dataset['train'],\n",
    "                  eval_dataset=tokenized_dataset['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee3d186",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117e95c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_model = AutoModelForSeq2SeqLM.from_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d34deaa",
   "metadata": {},
   "source": [
    "Evaluet the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326e5a69",
   "metadata": {},
   "source": [
    "Qualitative evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b12b83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 200\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "human_baseline_summary = dataset['test'][index]['summary']\n",
    "\n",
    "Prompt = f\"\"\"\n",
    "Summarize the following conversation:\n",
    "{dialogue}\n",
    "Summary:\n",
    "\"\"\"\n",
    "input_ids = tokenizer(Prompt, return_tensors='pt').input_ids\n",
    "original_model_outputs = original_model.generate(input_ids=input_ids, \n",
    "                                                 generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "original_model_text_outputs = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "instruct_model_outputs = instruct_model.generate(input_ids=input_ids, \n",
    "                                                 generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "instruct_model_text_outputs = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY: \\n{summary}')\n",
    "print(dash_line)\n",
    "print(f'ORIGINAL MODEL:\\n{original_model_text_outputs}')\n",
    "print(dash_line)\n",
    "print(f'INSTRUCT MODEL:\\n{instruct_model_text_outputs}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c42db0",
   "metadata": {},
   "source": [
    "Quantative Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c4e62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8cdcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogues = dataset['test'][0:10]['dialogue']\n",
    "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "\n",
    "for _, dialogue in enumerate(dialogues):\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation:\n",
    "{dialogue}\n",
    "Summary:\n",
    "\"\"\"\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "\n",
    "    original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    original_model_text_outputs = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "    original_model_summaries.append(original_model_text_outputs)\n",
    "\n",
    "    instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    instruct_model_text_outputs = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "    instruct_model_summaries.append(instruct_model_text_outputs)\n",
    "\n",
    "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries))\n",
    "\n",
    "df = pd.DataFrame(zipped_summaries, columns=['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b55302",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True\n",
    ")\n",
    "instruct_model_results = rouge.compute(\n",
    "    predictions=instruct_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True\n",
    ")\n",
    "print('ORIGINAL MODEL:\\n')\n",
    "print(f'{original_model_results}')\n",
    "print('NSTRUCT MODEL:\\n')\n",
    "print(f'{instruct_model_results}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9352c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Absolute percentage improvement of INSTRUCT MODEL over HUMAN BASELINE\")\n",
    "improvement = (np.array(list(instruct_model_results.values()))) - (np.array(list(original_model_results.values())))\n",
    "for key, value in zip(instruct_model_results, improvement):\n",
    "    print(f'{key}: {value*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393c1ea9",
   "metadata": {},
   "source": [
    "Parameter Efficient Fine Tuning (PEFT) with LoRA adapter layers/parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61f7ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ca8200",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "lora_config = LoraConfig(r=4,\n",
    "                         lora_alpha=8, \n",
    "                         target_modules=[\"q\", \"v\"],\n",
    "                         lora_dropout=0.05,\n",
    "                         bias=\"none\",\n",
    "                         task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834f0cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/flan-t5-small\"\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07eb844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm using FLAN-T5-small to avoid memory crash.\n",
    "\n",
    "peft_model = get_peft_model(original_model, lora_config)\n",
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d1e9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f'./peft-dialogue-summary-training-{str(int(time.time()))}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898c053b",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_training_args = TrainingArguments(output_dir=output_dir,\n",
    "                                       per_device_eval_batch_size=1, \n",
    "                                       auto_find_batch_size=True,\n",
    "                                       learning_rate=1e-3,\n",
    "                                       num_train_epochs=1,\n",
    "                                       logging_steps=1,\n",
    "                                       max_steps=1)\n",
    "peft_trainer = Trainer(model=peft_model, \n",
    "                       args=peft_training_args, \n",
    "                       train_dataset=tokenized_dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b58851",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_trainer.train()\n",
    "peft_model_path = \"./peft-dialogue-summary-checkpoint-local\"\n",
    "peft_trainer.model.save_pretrained(peft_model_path)\n",
    "tokenizer.save_pretrained(peft_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e792b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "peft_model_base = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abd2f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = PeftModel.from_pretrained(peft_model_base, \n",
    "                                       peft_model_path,\n",
    "                                       torch_dtype=torch.bfloat16,\n",
    "                                       is_trainable=False) # The goal is just to evaluate the model, just forwardpass, to minimize the footprint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5667fc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 200\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "human_baseline_summary = dataset['test'][index]['summary']\n",
    "\n",
    "Prompt = f\"\"\"\n",
    "Summarize the following conversation:\n",
    "{dialogue}\n",
    "Summary:\n",
    "\"\"\"\n",
    "input_ids = tokenizer(Prompt, return_tensors='pt').input_ids\n",
    "original_model_outputs = original_model.generate(input_ids=input_ids, \n",
    "                                                 generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "original_model_text_outputs = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "peft_model_outputs = peft_model.generate(input_ids=input_ids, \n",
    "                                                 generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "peft_model_text_outputs = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY: \\n{summary}')\n",
    "print(dash_line)\n",
    "print(f'ORIGINAL MODEL:\\n{original_model_text_outputs}')\n",
    "print(dash_line)\n",
    "print(f'INSTRUCT MODEL:\\n{peft_model_text_outputs}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415155ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogues = dataset['test'][0:10]['dialogue']\n",
    "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
    "original_model_summaries = []\n",
    "peft_model_summaries = []\n",
    "\n",
    "for _, dialogue in enumerate(dialogues):\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation:\n",
    "{dialogue}\n",
    "Summary:\n",
    "\"\"\"\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "\n",
    "    original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    original_model_text_outputs = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "    original_model_summaries.append(original_model_text_outputs)\n",
    "\n",
    "    peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    peft_model_text_outputs = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "    peft_model_summaries.append(peft_model_text_outputs)\n",
    "\n",
    "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, peft_model_summaries))\n",
    "\n",
    "df = pd.DataFrame(zipped_summaries, columns=['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae7c2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True\n",
    ")\n",
    "peft_model_results = rouge.compute(\n",
    "    predictions=peft_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(peft_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True\n",
    ")\n",
    "print('ORIGINAL MODEL:\\n')\n",
    "print(f'{original_model_results}')\n",
    "print('PEFT MODEL:\\n')\n",
    "print(f'{peft_model_results}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038662db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Absolute percentage improvement of INSTRUCT MODEL over HUMAN BASELINE\")\n",
    "improvement = (np.array(list(instruct_model_results.values()))) - (np.array(list(original_model_results.values())))\n",
    "for key, value in zip(instruct_model_results, improvement):\n",
    "    print(f'{key}: {value*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenAI_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
